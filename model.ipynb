{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./formattedMessages.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_NAME = 'Charlie Wyatt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexted = []\n",
    "\n",
    "# context window of size 7\n",
    "n = 7\n",
    "\n",
    "for i in data[data.name == CHARACTER_NAME].index:\n",
    "  if i < n:\n",
    "    continue\n",
    "  row = []\n",
    "  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n",
    "  for j in range(i, prev, -1):\n",
    "    row.append(data.line[j])\n",
    "  contexted.append(row)\n",
    "\n",
    "columns = ['response', 'context'] \n",
    "columns = columns + ['context/' + str(i) for i in range(n - 1)]\n",
    "\n",
    "df = pd.DataFrame.from_records(contexted, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29931</th>\n",
       "      <td>Sweet! What time do you guys think youll get here</td>\n",
       "      <td>All good! Tim Connors driving and weve found a...</td>\n",
       "      <td>Sweet!! I wont be able to help with transporta...</td>\n",
       "      <td>Yeah thatd be great!</td>\n",
       "      <td>So we dont have any indoor room, but we could ...</td>\n",
       "      <td>Sorry wrong button</td>\n",
       "      <td>Were getting the keys for our place on Monday ...</td>\n",
       "      <td>All good! I might be getting desk off Facebook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>I dont mind up to you, not sure about you but ...</td>\n",
       "      <td>Where do you want to meet?</td>\n",
       "      <td>Did you manage to snag me some shorts btw?</td>\n",
       "      <td>Yeah sounds great</td>\n",
       "      <td>Wanna go for 12?</td>\n",
       "      <td>Im free between 11-1?</td>\n",
       "      <td>Sure lets do it</td>\n",
       "      <td>Would you like to have a catch up over coffee ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11728</th>\n",
       "      <td>Thanks Carel, Ill let you know if I get anythi...</td>\n",
       "      <td>Oh yeah that makes lots of sense</td>\n",
       "      <td>Those are such nice looking visuals haha</td>\n",
       "      <td>Think you could avoid this and just half the f...</td>\n",
       "      <td>Since that represents the same pairing</td>\n",
       "      <td>but c21 doesn't go to p11</td>\n",
       "      <td>so c11 goes to p21</td>\n",
       "      <td>You can sort of see what I mean from the pic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57377</th>\n",
       "      <td>Were you thinking the coog one?</td>\n",
       "      <td>Kurtosh sounds great</td>\n",
       "      <td>I did some googling- would Kurtosh be fun?</td>\n",
       "      <td>Thoughts on a cafe?</td>\n",
       "      <td>Okay a bit of extra thinking time our light ra...</td>\n",
       "      <td>Sounds good</td>\n",
       "      <td>Oooh haha thats okay! Umm Im on the light rail...</td>\n",
       "      <td>Hmm yeah I would say super easy to come to min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80506</th>\n",
       "      <td>I think I shall get</td>\n",
       "      <td>But I think very worth it dollars</td>\n",
       "      <td>Yes it was many dollars</td>\n",
       "      <td>Is that also many dollars?</td>\n",
       "      <td>And in the large size</td>\n",
       "      <td>Also I have the pro</td>\n",
       "      <td>Yes many dollars</td>\n",
       "      <td>Lots of money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79227</th>\n",
       "      <td>Bump</td>\n",
       "      <td>Halo?</td>\n",
       "      <td>All good</td>\n",
       "      <td>Hmmm, not really feeling it right now sorry, w...</td>\n",
       "      <td>All good if not</td>\n",
       "      <td>Halo time?</td>\n",
       "      <td>Sweet as</td>\n",
       "      <td>Watching a talk atm and then breakfast but may...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  \\\n",
       "29931  Sweet! What time do you guys think youll get here   \n",
       "144    I dont mind up to you, not sure about you but ...   \n",
       "11728  Thanks Carel, Ill let you know if I get anythi...   \n",
       "57377                    Were you thinking the coog one?   \n",
       "80506                                I think I shall get   \n",
       "79227                                               Bump   \n",
       "\n",
       "                                                 context  \\\n",
       "29931  All good! Tim Connors driving and weve found a...   \n",
       "144                           Where do you want to meet?   \n",
       "11728                   Oh yeah that makes lots of sense   \n",
       "57377                               Kurtosh sounds great   \n",
       "80506                  But I think very worth it dollars   \n",
       "79227                                              Halo?   \n",
       "\n",
       "                                               context/0  \\\n",
       "29931  Sweet!! I wont be able to help with transporta...   \n",
       "144           Did you manage to snag me some shorts btw?   \n",
       "11728           Those are such nice looking visuals haha   \n",
       "57377         I did some googling- would Kurtosh be fun?   \n",
       "80506                            Yes it was many dollars   \n",
       "79227                                           All good   \n",
       "\n",
       "                                               context/1  \\\n",
       "29931                               Yeah thatd be great!   \n",
       "144                                    Yeah sounds great   \n",
       "11728  Think you could avoid this and just half the f...   \n",
       "57377                                Thoughts on a cafe?   \n",
       "80506                         Is that also many dollars?   \n",
       "79227  Hmmm, not really feeling it right now sorry, w...   \n",
       "\n",
       "                                               context/2  \\\n",
       "29931  So we dont have any indoor room, but we could ...   \n",
       "144                                     Wanna go for 12?   \n",
       "11728             Since that represents the same pairing   \n",
       "57377  Okay a bit of extra thinking time our light ra...   \n",
       "80506                              And in the large size   \n",
       "79227                                    All good if not   \n",
       "\n",
       "                       context/3  \\\n",
       "29931        Sorry wrong button    \n",
       "144        Im free between 11-1?   \n",
       "11728  but c21 doesn't go to p11   \n",
       "57377               Sounds good    \n",
       "80506        Also I have the pro   \n",
       "79227                 Halo time?   \n",
       "\n",
       "                                               context/4  \\\n",
       "29931  Were getting the keys for our place on Monday ...   \n",
       "144                                      Sure lets do it   \n",
       "11728                                 so c11 goes to p21   \n",
       "57377  Oooh haha thats okay! Umm Im on the light rail...   \n",
       "80506                                   Yes many dollars   \n",
       "79227                                           Sweet as   \n",
       "\n",
       "                                               context/5  \n",
       "29931  All good! I might be getting desk off Facebook...  \n",
       "144    Would you like to have a catch up over coffee ...  \n",
       "11728       You can sort of see what I mean from the pic  \n",
       "57377  Hmm yeah I would say super easy to come to min...  \n",
       "80506                                      Lots of money  \n",
       "79227  Watching a talk atm and then breakfast but may...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>context/0</th>\n",
       "      <th>context/1</th>\n",
       "      <th>context/2</th>\n",
       "      <th>context/3</th>\n",
       "      <th>context/4</th>\n",
       "      <th>context/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23532</th>\n",
       "      <td>Thanks dude</td>\n",
       "      <td>Oh true sf haha good luck</td>\n",
       "      <td>Sorry dude I have to write this application</td>\n",
       "      <td>Beach?</td>\n",
       "      <td>sounds good</td>\n",
       "      <td>Hmm Dino around 5.15 then head beach?</td>\n",
       "      <td>sure</td>\n",
       "      <td>Bus it there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70962</th>\n",
       "      <td>Love it</td>\n",
       "      <td>Gay cunt</td>\n",
       "      <td>That's mad</td>\n",
       "      <td>Lol</td>\n",
       "      <td>Alister Braham added Angus Webber to the group.</td>\n",
       "      <td>Alister Braham removed Gabby Wells from the gr...</td>\n",
       "      <td>Bye gab</td>\n",
       "      <td>She'll reck you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73556</th>\n",
       "      <td>Aghghgh cannot wait</td>\n",
       "      <td>Im about to have a shower but Ill look into it...</td>\n",
       "      <td>Wayyy better than I was thinking</td>\n",
       "      <td>Like anything with a beat works tbh</td>\n",
       "      <td>Right so we just tried the d floor with the ba...</td>\n",
       "      <td>I can hit ya up with my other shortlist as wel...</td>\n",
       "      <td>Let me know why you think so far? That should ...</td>\n",
       "      <td>Ive been tryna get some of the ol classics tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34369</th>\n",
       "      <td>Yeah jezza Ill be heading on the train on Satu...</td>\n",
       "      <td>Hows dubvegas treating you?</td>\n",
       "      <td>Ohh for sure its just always popping</td>\n",
       "      <td>I could never be sick of the promised land</td>\n",
       "      <td>Sounds good keep me posted. I bet youre sick o...</td>\n",
       "      <td>Ahh ok thats fair, I might actually end up com...</td>\n",
       "      <td>Millie(from basser) is coming on Friday to</td>\n",
       "      <td>Jungle giants that is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33655</th>\n",
       "      <td>Oooh yeah, Im in!</td>\n",
       "      <td>Yea doss house looks good! Will be there proba...</td>\n",
       "      <td>Thoughts on meeting at the Doss House?</td>\n",
       "      <td>Kieran Pulley added you to the group.</td>\n",
       "      <td>Amaris Moore removed Charlie Li from the group.</td>\n",
       "      <td>Amaris Moore removed a participant from the gr...</td>\n",
       "      <td>Amaris Moore removed Emily Dewar from the group.</td>\n",
       "      <td>Feel free to add anyone to the group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                response  \\\n",
       "23532                                        Thanks dude   \n",
       "70962                                            Love it   \n",
       "73556                                Aghghgh cannot wait   \n",
       "34369  Yeah jezza Ill be heading on the train on Satu...   \n",
       "33655                                  Oooh yeah, Im in!   \n",
       "\n",
       "                                                 context  \\\n",
       "23532                          Oh true sf haha good luck   \n",
       "70962                                           Gay cunt   \n",
       "73556  Im about to have a shower but Ill look into it...   \n",
       "34369                        Hows dubvegas treating you?   \n",
       "33655  Yea doss house looks good! Will be there proba...   \n",
       "\n",
       "                                         context/0  \\\n",
       "23532  Sorry dude I have to write this application   \n",
       "70962                                   That's mad   \n",
       "73556             Wayyy better than I was thinking   \n",
       "34369         Ohh for sure its just always popping   \n",
       "33655       Thoughts on meeting at the Doss House?   \n",
       "\n",
       "                                        context/1  \\\n",
       "23532                                      Beach?   \n",
       "70962                                         Lol   \n",
       "73556         Like anything with a beat works tbh   \n",
       "34369  I could never be sick of the promised land   \n",
       "33655       Kieran Pulley added you to the group.   \n",
       "\n",
       "                                               context/2  \\\n",
       "23532                                        sounds good   \n",
       "70962    Alister Braham added Angus Webber to the group.   \n",
       "73556  Right so we just tried the d floor with the ba...   \n",
       "34369  Sounds good keep me posted. I bet youre sick o...   \n",
       "33655    Amaris Moore removed Charlie Li from the group.   \n",
       "\n",
       "                                               context/3  \\\n",
       "23532              Hmm Dino around 5.15 then head beach?   \n",
       "70962  Alister Braham removed Gabby Wells from the gr...   \n",
       "73556  I can hit ya up with my other shortlist as wel...   \n",
       "34369  Ahh ok thats fair, I might actually end up com...   \n",
       "33655  Amaris Moore removed a participant from the gr...   \n",
       "\n",
       "                                               context/4  \\\n",
       "23532                                               sure   \n",
       "70962                                            Bye gab   \n",
       "73556  Let me know why you think so far? That should ...   \n",
       "34369         Millie(from basser) is coming on Friday to   \n",
       "33655   Amaris Moore removed Emily Dewar from the group.   \n",
       "\n",
       "                                               context/5  \n",
       "23532                                      Bus it there?  \n",
       "70962                                    She'll reck you  \n",
       "73556  Ive been tryna get some of the ol classics tha...  \n",
       "34369                              Jungle giants that is  \n",
       "33655               Feel free to add anyone to the group  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df, val_df = train_test_split(df, test_size=0.1)\n",
    "trn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset suitable for our model\n",
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    # print([x for x in row])\n",
    "    # print([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row])\n",
    "    # print(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(\n",
    "            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "        )\n",
    "\n",
    "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"rb\") as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            for _, row in df.iterrows():\n",
    "                conv = construct_conv(row, tokenizer)\n",
    "                self.examples.append(conv)\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, \"wb\") as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacheing and storing of data/checkpoints\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not args.save_total_limit:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1322: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output-small'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512 # this was 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 1\n",
    "        self.per_gpu_eval_batch_size = 1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 4\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runner\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    \n",
    "    model = AutoModelWithLMHead.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "   \n",
    "    model.to(args.device)\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2023 19:54:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/22/2023 19:54:14 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x0000016FAC462140>\n",
      "04/22/2023 19:54:14 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "04/22/2023 19:55:06 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "c:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "04/22/2023 19:55:06 - INFO - __main__ -   ***** Running training *****\n",
      "04/22/2023 19:55:06 - INFO - __main__ -     Num examples = 82056\n",
      "04/22/2023 19:55:06 - INFO - __main__ -     Num Epochs = 4\n",
      "04/22/2023 19:55:06 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
      "04/22/2023 19:55:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "04/22/2023 19:55:06 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "04/22/2023 19:55:06 - INFO - __main__ -     Total optimization steps = 328224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c2a9eacdc145a8a5f5b1aa61b2875c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e527ee6964f4ec3877b0f462206060a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/82056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "04/22/2023 19:58:59 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-3500\n",
      "04/22/2023 19:59:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-3500\n",
      "04/22/2023 20:02:52 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-7000\n",
      "04/22/2023 20:02:53 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-7000\n",
      "04/22/2023 20:06:45 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-10500\n",
      "04/22/2023 20:06:46 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-10500\n",
      "04/22/2023 20:10:38 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-14000\n",
      "04/22/2023 20:10:39 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-14000\n",
      "04/22/2023 20:14:31 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-17500\n",
      "04/22/2023 20:14:32 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-17500\n",
      "04/22/2023 20:18:25 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-21000\n",
      "04/22/2023 20:18:26 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-21000\n",
      "04/22/2023 20:22:20 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-24500\n",
      "04/22/2023 20:22:23 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-24500\n",
      "04/22/2023 20:26:15 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-28000\n",
      "04/22/2023 20:26:16 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-28000\n",
      "04/22/2023 20:30:08 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-31500\n",
      "04/22/2023 20:30:10 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-31500\n",
      "04/22/2023 20:34:02 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-35000\n",
      "04/22/2023 20:34:03 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-35000\n",
      "04/22/2023 20:37:56 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-38500\n",
      "04/22/2023 20:37:57 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-38500\n",
      "04/22/2023 20:41:49 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-42000\n",
      "04/22/2023 20:41:50 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-42000\n",
      "04/22/2023 20:45:44 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-45500\n",
      "04/22/2023 20:45:45 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-45500\n",
      "04/22/2023 20:49:38 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-49000\n",
      "04/22/2023 20:49:39 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-49000\n",
      "04/22/2023 20:53:31 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-52500\n",
      "04/22/2023 20:53:32 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-52500\n",
      "04/22/2023 20:57:25 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-56000\n",
      "04/22/2023 20:57:26 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-56000\n",
      "04/22/2023 21:01:18 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-59500\n",
      "04/22/2023 21:01:19 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-59500\n",
      "04/22/2023 21:05:12 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-63000\n",
      "04/22/2023 21:05:13 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-63000\n",
      "04/22/2023 21:09:05 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-66500\n",
      "04/22/2023 21:09:06 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-66500\n",
      "04/22/2023 21:12:59 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-70000\n",
      "04/22/2023 21:13:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-70000\n",
      "04/22/2023 21:16:53 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-73500\n",
      "04/22/2023 21:16:54 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-73500\n",
      "04/22/2023 21:20:48 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-77000\n",
      "04/22/2023 21:20:49 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-77000\n",
      "04/22/2023 21:24:41 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-80500\n",
      "04/22/2023 21:24:42 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-80500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9c7132fce94170914d1db4f508ce59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/82056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2023 21:28:34 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-84000\n",
      "04/22/2023 21:28:35 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-84000\n",
      "04/22/2023 21:32:27 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-87500\n",
      "04/22/2023 21:32:28 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-87500\n",
      "04/22/2023 21:36:21 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-91000\n",
      "04/22/2023 21:36:22 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-91000\n",
      "04/22/2023 21:40:14 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-94500\n",
      "04/22/2023 21:40:15 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-94500\n",
      "04/22/2023 21:44:08 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-98000\n",
      "04/22/2023 21:44:09 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-98000\n",
      "04/22/2023 21:48:02 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-101500\n",
      "04/22/2023 21:48:03 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-101500\n",
      "04/22/2023 21:51:55 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-105000\n",
      "04/22/2023 21:51:56 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-105000\n",
      "04/22/2023 21:55:49 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-108500\n",
      "04/22/2023 21:55:50 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-108500\n",
      "04/22/2023 21:59:43 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-112000\n",
      "04/22/2023 21:59:44 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-112000\n",
      "04/22/2023 22:03:46 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-115500\n",
      "04/22/2023 22:03:47 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-115500\n",
      "04/22/2023 22:07:58 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-119000\n",
      "04/22/2023 22:07:59 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-119000\n",
      "04/22/2023 22:12:12 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-122500\n",
      "04/22/2023 22:12:13 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-122500\n",
      "04/22/2023 22:16:28 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-126000\n",
      "04/22/2023 22:16:29 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-126000\n",
      "04/22/2023 22:20:42 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-129500\n",
      "04/22/2023 22:20:43 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-129500\n",
      "04/22/2023 22:24:55 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-133000\n",
      "04/22/2023 22:24:56 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-133000\n",
      "04/22/2023 22:29:09 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-136500\n",
      "04/22/2023 22:29:10 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-136500\n",
      "04/22/2023 22:33:06 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-140000\n",
      "04/22/2023 22:33:07 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-140000\n",
      "04/22/2023 22:36:59 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-143500\n",
      "04/22/2023 22:37:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-143500\n",
      "04/22/2023 22:40:53 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-147000\n",
      "04/22/2023 22:40:54 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-147000\n",
      "04/22/2023 22:44:51 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-150500\n",
      "04/22/2023 22:44:52 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-150500\n",
      "04/22/2023 22:48:45 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-154000\n",
      "04/22/2023 22:48:46 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-154000\n",
      "04/22/2023 22:52:38 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-157500\n",
      "04/22/2023 22:52:39 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-157500\n",
      "04/22/2023 22:56:31 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-161000\n",
      "04/22/2023 22:56:32 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-161000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b89f99e63f640aaa048ecb4fbdf7873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/82056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/22/2023 23:00:25 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-164500\n",
      "04/22/2023 23:00:26 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-164500\n",
      "04/22/2023 23:04:18 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-168000\n",
      "04/22/2023 23:04:19 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-168000\n",
      "04/22/2023 23:08:11 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-171500\n",
      "04/22/2023 23:08:12 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-171500\n",
      "04/22/2023 23:12:06 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-175000\n",
      "04/22/2023 23:12:07 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-175000\n",
      "04/22/2023 23:15:59 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-178500\n",
      "04/22/2023 23:16:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-178500\n",
      "04/22/2023 23:19:52 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-182000\n",
      "04/22/2023 23:19:53 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-182000\n",
      "04/22/2023 23:23:47 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-185500\n",
      "04/22/2023 23:23:48 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-185500\n",
      "04/22/2023 23:27:39 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-189000\n",
      "04/22/2023 23:27:40 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-189000\n",
      "04/22/2023 23:31:31 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-192500\n",
      "04/22/2023 23:31:32 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-192500\n",
      "04/22/2023 23:35:26 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-196000\n",
      "04/22/2023 23:35:27 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-196000\n",
      "04/22/2023 23:39:20 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-199500\n",
      "04/22/2023 23:39:21 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-199500\n",
      "04/22/2023 23:43:14 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-203000\n",
      "04/22/2023 23:43:15 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-203000\n",
      "04/22/2023 23:47:07 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-206500\n",
      "04/22/2023 23:47:08 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-206500\n",
      "04/22/2023 23:50:59 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-210000\n",
      "04/22/2023 23:51:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-210000\n",
      "04/22/2023 23:54:52 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-213500\n",
      "04/22/2023 23:54:53 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-213500\n",
      "04/22/2023 23:58:46 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-217000\n",
      "04/22/2023 23:58:47 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-217000\n",
      "04/23/2023 00:02:40 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-220500\n",
      "04/23/2023 00:02:41 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-220500\n",
      "04/23/2023 00:06:33 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-224000\n",
      "04/23/2023 00:06:34 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-224000\n",
      "04/23/2023 00:10:26 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-227500\n",
      "04/23/2023 00:10:26 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-227500\n",
      "04/23/2023 00:14:19 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-231000\n",
      "04/23/2023 00:14:20 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-231000\n",
      "04/23/2023 00:18:13 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-234500\n",
      "04/23/2023 00:18:14 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-234500\n",
      "04/23/2023 00:22:03 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-238000\n",
      "04/23/2023 00:22:04 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-238000\n",
      "04/23/2023 00:25:54 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-241500\n",
      "04/23/2023 00:25:55 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-241500\n",
      "04/23/2023 00:29:43 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-245000\n",
      "04/23/2023 00:29:44 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-245000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c8b6b63b7a4d60bfc2faae84bfa5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/82056 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/23/2023 00:33:33 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-248500\n",
      "04/23/2023 00:33:34 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-248500\n",
      "04/23/2023 00:37:24 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-252000\n",
      "04/23/2023 00:37:25 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-252000\n",
      "04/23/2023 00:41:14 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-255500\n",
      "04/23/2023 00:41:15 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-255500\n",
      "04/23/2023 00:45:04 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-259000\n",
      "04/23/2023 00:45:05 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-259000\n",
      "04/23/2023 00:48:55 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-262500\n",
      "04/23/2023 00:48:55 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-262500\n",
      "04/23/2023 00:52:45 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-266000\n",
      "04/23/2023 00:52:46 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-266000\n",
      "04/23/2023 00:56:34 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-269500\n",
      "04/23/2023 00:56:35 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-269500\n",
      "04/23/2023 01:00:24 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-273000\n",
      "04/23/2023 01:00:25 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-273000\n",
      "04/23/2023 01:04:15 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-276500\n",
      "04/23/2023 01:04:16 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-276500\n",
      "04/23/2023 01:08:05 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-280000\n",
      "04/23/2023 01:08:06 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-280000\n",
      "04/23/2023 01:11:54 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-283500\n",
      "04/23/2023 01:11:55 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-283500\n",
      "04/23/2023 01:15:45 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-287000\n",
      "04/23/2023 01:15:46 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-287000\n",
      "04/23/2023 01:19:35 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-290500\n",
      "04/23/2023 01:19:36 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-290500\n",
      "04/23/2023 01:23:25 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-294000\n",
      "04/23/2023 01:23:26 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-294000\n",
      "04/23/2023 01:27:15 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-297500\n",
      "04/23/2023 01:27:16 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-297500\n",
      "04/23/2023 01:31:05 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-301000\n",
      "04/23/2023 01:31:06 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-301000\n",
      "04/23/2023 01:34:56 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-304500\n",
      "04/23/2023 01:34:57 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-304500\n",
      "04/23/2023 01:38:46 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-308000\n",
      "04/23/2023 01:38:47 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-308000\n",
      "04/23/2023 01:42:37 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-311500\n",
      "04/23/2023 01:42:38 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-311500\n",
      "04/23/2023 01:46:27 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-315000\n",
      "04/23/2023 01:46:28 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-315000\n",
      "04/23/2023 01:50:17 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-318500\n",
      "04/23/2023 01:50:18 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-318500\n",
      "04/23/2023 01:54:08 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-322000\n",
      "04/23/2023 01:54:09 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-322000\n",
      "04/23/2023 01:57:59 - INFO - __main__ -   Saving model checkpoint to output-small\\checkpoint-325500\n",
      "04/23/2023 01:58:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small\\checkpoint-325500\n",
      "04/23/2023 02:00:48 - INFO - __main__ -    global_step = 328064, average loss = 2.7094700838017327\n",
      "04/23/2023 02:00:48 - INFO - __main__ -   Saving model checkpoint to output-small\n",
      "04/23/2023 02:00:49 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small']\n",
      "04/23/2023 02:00:51 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7194 > 1024). Running this sequence through the model will result in indexing errors\n",
      "04/23/2023 02:00:57 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_512\n",
      "04/23/2023 02:00:57 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "04/23/2023 02:00:57 - INFO - __main__ -     Num examples = 9118\n",
      "04/23/2023 02:00:57 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49693fd35c14b4f875169217c82806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\charl\\Documents\\Projects\\Facebook Imitator\\model.ipynb Cell 14\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main(trn_df, val_df)\n",
      "\u001b[1;32mc:\\Users\\charl\\Documents\\Projects\\Facebook Imitator\\model.ipynb Cell 14\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelWithLMHead\u001b[39m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m model\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m result \u001b[39m=\u001b[39m evaluate(args, model, tokenizer, df_trn, df_val, prefix\u001b[39m=\u001b[39;49mprefix)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((k \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(global_step), v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m results\u001b[39m.\u001b[39mupdate(result)\n",
      "\u001b[1;32mc:\\Users\\charl\\Documents\\Projects\\Facebook Imitator\\model.ipynb Cell 14\u001b[0m in \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m     lm_loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/charl/Documents/Projects/Facebook%20Imitator/model.ipynb#X16sZmlsZQ%3D%3D?line=233'>234</a>\u001b[0m     eval_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m lm_loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1075\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1076\u001b[0m     input_ids,\n\u001b[0;32m   1077\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1078\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1079\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1080\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1081\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1082\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1083\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1084\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1085\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1086\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1090\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1092\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    889\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    897\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    900\u001b[0m         hidden_states,\n\u001b[0;32m    901\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    902\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    903\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    904\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    905\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    906\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    907\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    908\u001b[0m     )\n\u001b[0;32m    910\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    387\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 389\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    390\u001b[0m     hidden_states,\n\u001b[0;32m    391\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    392\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    393\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    394\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    395\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    396\u001b[0m )\n\u001b[0;32m    397\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    398\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:330\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    332\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\charl\\anaconda3\\envs\\comp9418\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:182\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_attn\u001b[39m(\u001b[39mself\u001b[39m, query, key, value, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, head_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 182\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query, key\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    184\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[0;32m    185\u001b[0m         attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mfull(\n\u001b[0;32m    186\u001b[0m             [], value\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m, dtype\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mattn_weights\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    187\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "main(trn_df, val_df) # test dataset had a perplexity of 9.2987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained('./output-small/checkpoint-325500/') # TO DO: CHANGE THIS TO THE BEST TRAINED SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charliebot: Haha no worries, I think I am going to be at gerty anyway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charliebot: Yeah sure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charliebot: Hey Xanthe are you around?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charliebot: Xavier called you.\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 4 lines\n",
    "for step in range(4):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # print(new_user_input_ids)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,       \n",
    "        do_sample=True, \n",
    "        top_k=100, \n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Charliebot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9418",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
